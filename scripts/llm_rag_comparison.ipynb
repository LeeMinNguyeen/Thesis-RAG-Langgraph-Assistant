{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f78cba6",
   "metadata": {},
   "source": [
    "# LLM RAG Accuracy Comparison\n",
    "\n",
    "This notebook compares multiple LLM models on RAG (Retrieval-Augmented Generation) accuracy using:\n",
    "- **Embedding**: Gemini `text-embedding-004` (768 dimensions)\n",
    "- **Vector Store**: MongoDB Atlas with cosine similarity\n",
    "- **Fact Table**: 20 Q&A pairs from UEL (University of Economics and Law) documents\n",
    "\n",
    "## Prerequisites\n",
    "Before running this notebook, ensure you have:\n",
    "1. Run `scripts/embed_pdfs_to_mongo.py` to embed PDFs into MongoDB\n",
    "2. Created a vector search index named `vector_index` in MongoDB Atlas\n",
    "3. Set up `.env` with `GEMINI_API_KEY`, `GROQ_API_KEY`, `MONGODB_URI2`, `MONGODB_DB_NAME2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a9661f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install google-genai groq langchain-mongodb langchain-google-genai pymongo python-dotenv pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5665a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded successfully!\n",
      "MongoDB DB: thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI2\")\n",
    "MONGODB_DB_NAME = os.getenv(\"MONGODB_DB_NAME2\")\n",
    "COLLECTION_NAME = \"pdf\"\n",
    "INDEX_NAME = \"vector_index\"\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(f\"MongoDB DB: {MONGODB_DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af1d1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: c:\\Projects\\Thesis-RAG-Langgraph-Assistant\\scripts\\fact_table.json\n",
      "Loaded 10 questions from fact table\n",
      "\n",
      "Q1: ThÆ° viá»‡n UELlib cÃ³ bao nhiÃªu báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025)?...\n",
      "A: ThÆ° viá»‡n UELlib cÃ³ nguá»“n tÃ i nguyÃªn thÃ´ng tin Ä‘a dáº¡ng, vá»›i 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025)....\n",
      "\n",
      "Q2: Äá»‹a chá»‰ cá»§a ThÆ° viá»‡n TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t lÃ  gÃ¬?...\n",
      "A: Äá»‹a chá»‰: Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM. ThÆ° viá»‡n Ä‘áº·t táº¡i tÃ²a nhÃ  KTL B2 (táº§n...\n",
      "\n",
      "Q3: NÄƒm há»c 2025-2026, UEL cÃ³ bao nhiÃªu ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c?...\n",
      "A: NÄƒm há»c 2025-2026, UEL cÃ³ 35 ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c, trong Ä‘Ã³ 24 chÆ°Æ¡ng trÃ¬nh dáº¡y vÃ  há»c...\n"
     ]
    }
   ],
   "source": [
    "# Load fact table\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the fact table (same directory as notebook)\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else os.getcwd()\n",
    "FACT_TABLE_PATH = os.path.join(SCRIPT_DIR, \"fact_table.json\")\n",
    "\n",
    "# Fallback paths\n",
    "if not os.path.exists(FACT_TABLE_PATH):\n",
    "    FACT_TABLE_PATH = \"fact_table.json\"  # Same directory\n",
    "if not os.path.exists(FACT_TABLE_PATH):\n",
    "    FACT_TABLE_PATH = \"scripts/fact_table.json\"  # From project root\n",
    "if not os.path.exists(FACT_TABLE_PATH):\n",
    "    FACT_TABLE_PATH = r\"c:\\Projects\\Thesis-RAG-Langgraph-Assistant\\scripts\\fact_table.json\"  # Absolute path\n",
    "\n",
    "print(f\"Loading from: {FACT_TABLE_PATH}\")\n",
    "\n",
    "with open(FACT_TABLE_PATH, 'r', encoding='utf-8') as f:\n",
    "    fact_table = json.load(f)\n",
    "\n",
    "questions_and_answers = fact_table[\"questions_and_answers\"]\n",
    "print(f\"Loaded {len(questions_and_answers)} questions from fact table\")\n",
    "\n",
    "# Display sample\n",
    "for qa in questions_and_answers[:3]:\n",
    "    print(f\"\\nQ{qa['id']}: {qa['question'][:80]}...\")\n",
    "    print(f\"A: {qa['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f4695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized!\n",
      "Documents in collection: 403\n"
     ]
    }
   ],
   "source": [
    "# Setup MongoDB Vector Store for RAG\n",
    "client = MongoClient(MONGODB_URI)\n",
    "collection = client[MONGODB_DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=collection,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\",\n",
    "    text_key=\"text\"  # Match the field name used in embed_pdfs_to_mongo.py\n",
    ")\n",
    "\n",
    "print(\"Vector store initialized!\")\n",
    "print(f\"Documents in collection: {collection.count_documents({})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "118252d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context length: 3787 characters\n",
      "Preview: tÄ©nh,\n",
      "\n",
      "phÃ²ng\n",
      "\n",
      "há»c\n",
      "\n",
      "nhÃ³m ThÆ° viá»‡n UELlib cÃ³ nguá»“n tÃ i nguyÃªn thÃ´ng tin Ä‘a dáº¡ng, vá»›i 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025) tráº£i\n",
      "\n",
      "Ä‘á»u\n",
      "\n",
      "á»Ÿ\n",
      "\n",
      "cÃ¡c\n",
      "\n",
      "ngÃ nh\n",
      "\n",
      "há»c\n",
      "\n",
      "cÃ¹ng\n",
      "\n",
      "vá»›i\n",
      "\n",
      "cÃ¡c\n",
      "\n",
      "lÄ©nh\n",
      "\n",
      "vá»±c\n",
      "\n",
      "khÃ¡c,\n",
      "\n",
      "hÃ ng\n",
      "\n",
      "trÄƒm\n",
      "\n",
      "triá»‡u\n",
      "\n",
      "tÃ i\n",
      "\n",
      "nguyÃªn\n",
      "\n",
      "thÃ´ng\n",
      "\n",
      "tin\n",
      "\n",
      "Ä‘iá»‡n\n",
      "\n",
      "tá»­\n",
      "\n",
      "gá»“m\n",
      "\n",
      "bÃ i\n",
      "\n",
      "bÃ¡o\n",
      "\n",
      "nghiÃªn\n",
      "\n",
      "cá»©u,\n",
      "\n",
      "sÃ¡ch,\n",
      "\n",
      "táº¡p\n",
      "\n",
      "chÃ­,\n",
      "\n",
      "luáº­n\n",
      "\n",
      "vÄƒn-luáº­n\n",
      "\n",
      "Ã¡n\n",
      "\n",
      "vÃ \n",
      "\n",
      "nhiá»u\n",
      "\n",
      "tÃ i\n",
      "\n",
      "liá»‡u\n",
      "\n",
      "khoa\n",
      "\n",
      "há»c\n",
      "\n",
      "khÃ¡c\n",
      "\n",
      "Ä‘Æ°á»£c\n",
      "\n",
      "táº­p\n",
      "\n",
      "há»£p\n",
      "\n",
      "tá»«\n",
      "\n",
      "cÃ¡c\n",
      "\n",
      "NhÃ \n",
      "\n",
      "xuáº¥t\n",
      "\n",
      "báº£n\n",
      "\n",
      "uy\n",
      "\n",
      "tÃ­n\n",
      "\n",
      "trÃªn\n",
      "\n",
      "tháº¿\n",
      "\n",
      "giá»›i\n",
      "\n",
      "vÃ \n",
      "\n",
      "trong\n",
      "\n",
      "nÆ°á»›c.\n",
      "\n",
      "Trong\n",
      "\n",
      "Ä‘Ã³\n",
      "\n",
      "bao\n",
      "\n",
      "gá»“m\n",
      "\n",
      "kho\n",
      "\n",
      "thÃ´ng\n",
      "\n",
      "tin\n",
      "\n",
      "khoa\n",
      "\n",
      "há»c\n",
      "\n",
      "sá»‘\n",
      "\n",
      "cá»§a\n",
      "\n",
      "cÃ¡c\n",
      "\n",
      "trÆ°...\n"
     ]
    }
   ],
   "source": [
    "def retrieve_context(question: str, k: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant context from vector store.\"\"\"\n",
    "    docs = vector_store.similarity_search(question, k=k)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return context\n",
    "\n",
    "# Test retrieval\n",
    "test_context = retrieve_context(questions_and_answers[0][\"question\"])\n",
    "print(f\"Retrieved context length: {len(test_context)} characters\")\n",
    "print(f\"Preview: {test_context[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d24946ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM clients initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM clients\n",
    "from google import genai\n",
    "from groq import Groq\n",
    "\n",
    "# Gemini client\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Groq client\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "print(\"LLM clients initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f752f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 6 models for testing\n"
     ]
    }
   ],
   "source": [
    "# Define model configurations\n",
    "MODELS = {\n",
    "    \"Gemini 2.5 Flash\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model_id\": \"gemini-2.5-flash\"\n",
    "    },\n",
    "    \"Gemini 2.5 Pro\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model_id\": \"gemini-2.5-pro\"\n",
    "    },\n",
    "    \"Llama 3.3 70B\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_id\": \"llama-3.3-70b-versatile\"\n",
    "    },\n",
    "    \"GPT-OSS 120B\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_id\": \"openai/gpt-oss-120b\"\n",
    "    },\n",
    "    \"Kimi K2\": {\n",
    "        \"provider\": \"groq\",\n",
    "        \"model_id\": \"moonshotai/kimi-k2-instruct-0905\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(MODELS)} models for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af9e77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query functions defined!\n"
     ]
    }
   ],
   "source": [
    "def create_rag_prompt(question: str, context: str) -> str:\n",
    "    \"\"\"Create a RAG prompt with context and question.\"\"\"\n",
    "    return f\"\"\"Dá»±a vÃ o ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  ngáº¯n gá»n.\n",
    "Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« ngá»¯ cáº£nh Ä‘á»ƒ tráº£ lá»i. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y nÃ³i \"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin\".\n",
    "\n",
    "NGá»® Cáº¢NH:\n",
    "{context}\n",
    "\n",
    "CÃ‚U Há»I: {question}\n",
    "\n",
    "TRáº¢ Lá»œI:\"\"\"\n",
    "\n",
    "\n",
    "def query_gemini(prompt: str, model_id: str) -> str:\n",
    "    \"\"\"Query Gemini model.\"\"\"\n",
    "    try:\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=model_id,\n",
    "            contents=prompt\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def query_groq(prompt: str, model_id: str) -> str:\n",
    "    \"\"\"Query Groq model.\"\"\"\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def query_model(question: str, context: str, model_name: str) -> str:\n",
    "    \"\"\"Query a model with RAG context.\"\"\"\n",
    "    model_config = MODELS[model_name]\n",
    "    prompt = create_rag_prompt(question, context)\n",
    "    \n",
    "    if model_config[\"provider\"] == \"gemini\":\n",
    "        return query_gemini(prompt, model_config[\"model_id\"])\n",
    "    else:  # groq\n",
    "        return query_groq(prompt, model_config[\"model_id\"])\n",
    "\n",
    "print(\"Query functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e39179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove punctuation at end\n",
    "    text = re.sub(r'[.,:;!?]+$', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_key_facts(answer: str) -> List[str]:\n",
    "    \"\"\"Extract key facts from an answer (numbers, names, dates).\"\"\"\n",
    "    facts = []\n",
    "    \n",
    "    # Extract numbers (including decimals and percentages)\n",
    "    numbers = re.findall(r'\\d+[.,]?\\d*%?', answer)\n",
    "    facts.extend(numbers)\n",
    "    \n",
    "    # Extract Vietnamese proper nouns (capitalized words)\n",
    "    # Look for patterns like \"PGS.TS\", \"ThS.\", names, etc.\n",
    "    names = re.findall(r'(?:PGS\\.|TS\\.|ThS\\.|GS\\.)?\\s*[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*(?:\\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]*)+', answer)\n",
    "    facts.extend([n.strip() for n in names if len(n.strip()) > 3])\n",
    "    \n",
    "    return facts\n",
    "\n",
    "\n",
    "def calculate_accuracy(model_answer: str, ground_truth: str) -> float:\n",
    "    \"\"\"Calculate accuracy score between model answer and ground truth.\"\"\"\n",
    "    if not model_answer or \"Error\" in model_answer or \"KhÃ´ng tÃ¬m tháº¥y\" in model_answer:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract key facts from both\n",
    "    gt_facts = extract_key_facts(ground_truth)\n",
    "    model_facts = extract_key_facts(model_answer)\n",
    "    \n",
    "    if not gt_facts:\n",
    "        # If no key facts, use text similarity\n",
    "        gt_normalized = normalize_text(ground_truth)\n",
    "        model_normalized = normalize_text(model_answer)\n",
    "        \n",
    "        # Check if key phrases from ground truth appear in model answer\n",
    "        gt_words = set(gt_normalized.split())\n",
    "        model_words = set(model_normalized.split())\n",
    "        \n",
    "        if not gt_words:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(gt_words.intersection(model_words))\n",
    "        return min(overlap / len(gt_words), 1.0)\n",
    "    \n",
    "    # Count how many ground truth facts appear in model answer\n",
    "    matches = 0\n",
    "    model_answer_lower = model_answer.lower()\n",
    "    \n",
    "    for fact in gt_facts:\n",
    "        if fact.lower() in model_answer_lower:\n",
    "            matches += 1\n",
    "    \n",
    "    return matches / len(gt_facts) if gt_facts else 0.0\n",
    "\n",
    "\n",
    "# Test accuracy calculation\n",
    "test_gt = \"TrÆ°á»ng cÃ³ 428 nhÃ¢n sá»±, trong Ä‘Ã³ 48,5% cÃ³ trÃ¬nh Ä‘á»™ TS, PGS vÃ  GS\"\n",
    "test_model = \"Theo thÃ´ng tin, trÆ°á»ng cÃ³ 428 nhÃ¢n sá»± vá»›i 48,5% cÃ³ trÃ¬nh Ä‘á»™ Tiáº¿n sÄ© trá»Ÿ lÃªn.\"\n",
    "print(f\"Test accuracy: {calculate_accuracy(test_model, test_gt):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3ac0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 10 questions...\n",
      "============================================================\n",
      "\n",
      "Question 1/10: ThÆ° viá»‡n UELlib cÃ³ bao nhiÃªu báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/202...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 100.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 100.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 50.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 50.00%\n",
      "\n",
      "Question 2/10: Äá»‹a chá»‰ cá»§a ThÆ° viá»‡n TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t lÃ  gÃ¬?...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 57.14%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 57.14%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 57.14%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 28.57%\n",
      "  - Querying Qwen3 32B... Accuracy: 85.71%\n",
      "  - Querying Kimi K2... Accuracy: 57.14%\n",
      "\n",
      "Question 3/10: NÄƒm há»c 2025-2026, UEL cÃ³ bao nhiÃªu ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o t...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 66.67%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 66.67%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 66.67%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 16.67%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 16.67%\n",
      "\n",
      "Question 4/10: TÃ­nh Ä‘áº¿n giá»¯a nÄƒm 2025, TrÆ°á»ng UEL cÃ³ bao nhiÃªu nhÃ¢n sá»± vÃ  t...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 75.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 75.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 75.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 25.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 75.00%\n",
      "  - Querying Kimi K2... Accuracy: 75.00%\n",
      "\n",
      "Question 5/10: Sinh viÃªn bá»‹ ká»· luáº­t má»©c khiá»ƒn trÃ¡ch thÃ¬ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ káº¿t q...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 61.90%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 80.95%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 28.57%\n",
      "  - Querying Qwen3 32B... Accuracy: 76.19%\n",
      "  - Querying Kimi K2... Accuracy: 4.76%\n",
      "\n",
      "Question 6/10: Má»©c há»c bá»•ng loáº¡i giá»i báº±ng bao nhiÃªu pháº§n trÄƒm so vá»›i má»©c h...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 100.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 100.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 100.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 100.00%\n",
      "\n",
      "Question 7/10: Má»©c há»c bá»•ng loáº¡i xuáº¥t sáº¯c báº±ng bao nhiÃªu pháº§n trÄƒm so vá»›i m...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 100.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 100.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 0.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 100.00%\n",
      "\n",
      "Question 8/10: UEL cÃ³ bao nhiÃªu ngÃ nh thuá»™c Top 500 trÃªn báº£ng xáº¿p háº¡ng QS W...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 100.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 100.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 75.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 100.00%\n",
      "\n",
      "Question 9/10: UEL Ä‘Ã£ cung cáº¥p cho xÃ£ há»™i bao nhiÃªu cá»­ nhÃ¢n, tháº¡c sÄ©, tiáº¿n ...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 50.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 50.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 50.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 50.00%\n",
      "\n",
      "Question 10/10: TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t Ä‘Ã£ nháº­n Ä‘Æ°á»£c nhá»¯ng HuÃ¢n chÆ°Æ¡ng...\n",
      "  - Querying Gemini 2.5 Flash... Accuracy: 100.00%\n",
      "  - Querying Gemini 2.5 Pro... Accuracy: 100.00%\n",
      "  - Querying Llama 3.3 70B... Accuracy: 100.00%\n",
      "  - Querying GPT-OSS 120B... Accuracy: 100.00%\n",
      "  - Querying Qwen3 32B... Accuracy: 100.00%\n",
      "  - Querying Kimi K2... Accuracy: 100.00%\n",
      "\n",
      "============================================================\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation on all models\n",
    "results = {model_name: [] for model_name in MODELS.keys()}\n",
    "detailed_results = []\n",
    "\n",
    "# Limit questions for testing (remove this line to run all 20)\n",
    "test_questions = questions_and_answers  # Use all 20 questions\n",
    "\n",
    "print(f\"Running evaluation on {len(test_questions)} questions...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, qa in enumerate(test_questions):\n",
    "    question = qa[\"question\"]\n",
    "    ground_truth = qa[\"answer\"]\n",
    "    q_id = qa[\"id\"]\n",
    "    \n",
    "    print(f\"\\nQuestion {q_id}/{len(test_questions)}: {question[:60]}...\")\n",
    "    \n",
    "    # Retrieve context once for all models\n",
    "    context = retrieve_context(question)\n",
    "    \n",
    "    for model_name in MODELS.keys():\n",
    "        print(f\"  - Querying {model_name}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            # Query model\n",
    "            model_answer = query_model(question, context, model_name)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = calculate_accuracy(model_answer, ground_truth)\n",
    "            results[model_name].append(accuracy)\n",
    "            \n",
    "            # Store detailed result\n",
    "            detailed_results.append({\n",
    "                \"question_id\": q_id,\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"model\": model_name,\n",
    "                \"model_answer\": model_answer,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.2%}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results[model_name].append(0.0)\n",
    "            detailed_results.append({\n",
    "                \"question_id\": q_id,\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"model\": model_name,\n",
    "                \"model_answer\": f\"Error: {e}\",\n",
    "                \"accuracy\": 0.0\n",
    "            })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00c1bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL ACCURACY COMPARISON\n",
      "================================================================================\n",
      "           Model Average Accuracy Max Accuracy Min Accuracy  Perfect Scores (â‰¥90%)  Questions Tested\n",
      "       Qwen3 32B            93.7%       100.0%        75.0%                      7                10\n",
      "Gemini 2.5 Flash            89.9%       100.0%        57.1%                      7                10\n",
      "   Llama 3.3 70B            83.0%       100.0%        50.0%                      5                10\n",
      "  Gemini 2.5 Pro            81.1%       100.0%        50.0%                      5                10\n",
      "         Kimi K2            65.4%       100.0%         4.8%                      4                10\n",
      "    GPT-OSS 120B            47.4%       100.0%         0.0%                      2                10\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "summary_data = []\n",
    "\n",
    "for model_name, accuracies in results.items():\n",
    "    if accuracies:\n",
    "        avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "        max_accuracy = max(accuracies)\n",
    "        min_accuracy = min(accuracies)\n",
    "        perfect_scores = sum(1 for a in accuracies if a >= 0.9)\n",
    "    else:\n",
    "        avg_accuracy = max_accuracy = min_accuracy = 0.0\n",
    "        perfect_scores = 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Average Accuracy\": f\"{avg_accuracy:.1%}\",\n",
    "        \"Max Accuracy\": f\"{max_accuracy:.1%}\",\n",
    "        \"Min Accuracy\": f\"{min_accuracy:.1%}\",\n",
    "        \"Perfect Scores (â‰¥90%)\": perfect_scores,\n",
    "        \"Questions Tested\": len(accuracies)\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values(\"Average Accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL ACCURACY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61420dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0bd6a_row0_col0, #T_0bd6a_row0_col3, #T_0bd6a_row9_col0, #T_0bd6a_row9_col2, #T_0bd6a_row9_col3, #T_0bd6a_row9_col4 {\n",
       "  background-color: #feffbe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row0_col1, #T_0bd6a_row0_col2, #T_0bd6a_row0_col4, #T_0bd6a_row0_col5, #T_0bd6a_row1_col0, #T_0bd6a_row1_col1, #T_0bd6a_row1_col2, #T_0bd6a_row1_col3, #T_0bd6a_row1_col4, #T_0bd6a_row1_col5, #T_0bd6a_row3_col5, #T_0bd6a_row5_col1, #T_0bd6a_row6_col0, #T_0bd6a_row6_col1, #T_0bd6a_row6_col2, #T_0bd6a_row6_col3, #T_0bd6a_row6_col4, #T_0bd6a_row6_col5, #T_0bd6a_row7_col1, #T_0bd6a_row7_col2, #T_0bd6a_row7_col3, #T_0bd6a_row7_col4, #T_0bd6a_row7_col5, #T_0bd6a_row8_col1, #T_0bd6a_row8_col2, #T_0bd6a_row8_col3, #T_0bd6a_row8_col4, #T_0bd6a_row8_col5, #T_0bd6a_row9_col1, #T_0bd6a_row9_col5 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row2_col0, #T_0bd6a_row5_col0 {\n",
       "  background-color: #fca55d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row2_col1, #T_0bd6a_row2_col2, #T_0bd6a_row2_col3, #T_0bd6a_row2_col4 {\n",
       "  background-color: #e3f399;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row2_col5 {\n",
       "  background-color: #39a758;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row3_col0, #T_0bd6a_row3_col3 {\n",
       "  background-color: #ea5739;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row3_col1, #T_0bd6a_row3_col2, #T_0bd6a_row3_col4 {\n",
       "  background-color: #b7e075;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row4_col0 {\n",
       "  background-color: #f98e52;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row4_col1, #T_0bd6a_row4_col2, #T_0bd6a_row4_col3, #T_0bd6a_row4_col4, #T_0bd6a_row4_col5, #T_0bd6a_row8_col0 {\n",
       "  background-color: #84ca66;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row5_col2 {\n",
       "  background-color: #cfeb85;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row5_col3 {\n",
       "  background-color: #bd1726;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row5_col4 {\n",
       "  background-color: #5db961;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0bd6a_row5_col5 {\n",
       "  background-color: #7dc765;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0bd6a_row7_col0 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0bd6a\">\n",
       "  <caption>Accuracy by Question and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th id=\"T_0bd6a_level0_col0\" class=\"col_heading level0 col0\" >GPT-OSS 120B</th>\n",
       "      <th id=\"T_0bd6a_level0_col1\" class=\"col_heading level0 col1\" >Gemini 2.5 Flash</th>\n",
       "      <th id=\"T_0bd6a_level0_col2\" class=\"col_heading level0 col2\" >Gemini 2.5 Pro</th>\n",
       "      <th id=\"T_0bd6a_level0_col3\" class=\"col_heading level0 col3\" >Kimi K2</th>\n",
       "      <th id=\"T_0bd6a_level0_col4\" class=\"col_heading level0 col4\" >Llama 3.3 70B</th>\n",
       "      <th id=\"T_0bd6a_level0_col5\" class=\"col_heading level0 col5\" >Qwen3 32B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Question</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row0\" class=\"row_heading level0 row0\" >Q1</th>\n",
       "      <td id=\"T_0bd6a_row0_col0\" class=\"data row0 col0\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row0_col1\" class=\"data row0 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row0_col2\" class=\"data row0 col2\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row0_col3\" class=\"data row0 col3\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row0_col4\" class=\"data row0 col4\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row0_col5\" class=\"data row0 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row1\" class=\"row_heading level0 row1\" >Q10</th>\n",
       "      <td id=\"T_0bd6a_row1_col0\" class=\"data row1 col0\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row1_col1\" class=\"data row1 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row1_col2\" class=\"data row1 col2\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row1_col3\" class=\"data row1 col3\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row1_col4\" class=\"data row1 col4\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row1_col5\" class=\"data row1 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row2\" class=\"row_heading level0 row2\" >Q2</th>\n",
       "      <td id=\"T_0bd6a_row2_col0\" class=\"data row2 col0\" >29%</td>\n",
       "      <td id=\"T_0bd6a_row2_col1\" class=\"data row2 col1\" >57%</td>\n",
       "      <td id=\"T_0bd6a_row2_col2\" class=\"data row2 col2\" >57%</td>\n",
       "      <td id=\"T_0bd6a_row2_col3\" class=\"data row2 col3\" >57%</td>\n",
       "      <td id=\"T_0bd6a_row2_col4\" class=\"data row2 col4\" >57%</td>\n",
       "      <td id=\"T_0bd6a_row2_col5\" class=\"data row2 col5\" >86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row3\" class=\"row_heading level0 row3\" >Q3</th>\n",
       "      <td id=\"T_0bd6a_row3_col0\" class=\"data row3 col0\" >17%</td>\n",
       "      <td id=\"T_0bd6a_row3_col1\" class=\"data row3 col1\" >67%</td>\n",
       "      <td id=\"T_0bd6a_row3_col2\" class=\"data row3 col2\" >67%</td>\n",
       "      <td id=\"T_0bd6a_row3_col3\" class=\"data row3 col3\" >17%</td>\n",
       "      <td id=\"T_0bd6a_row3_col4\" class=\"data row3 col4\" >67%</td>\n",
       "      <td id=\"T_0bd6a_row3_col5\" class=\"data row3 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row4\" class=\"row_heading level0 row4\" >Q4</th>\n",
       "      <td id=\"T_0bd6a_row4_col0\" class=\"data row4 col0\" >25%</td>\n",
       "      <td id=\"T_0bd6a_row4_col1\" class=\"data row4 col1\" >75%</td>\n",
       "      <td id=\"T_0bd6a_row4_col2\" class=\"data row4 col2\" >75%</td>\n",
       "      <td id=\"T_0bd6a_row4_col3\" class=\"data row4 col3\" >75%</td>\n",
       "      <td id=\"T_0bd6a_row4_col4\" class=\"data row4 col4\" >75%</td>\n",
       "      <td id=\"T_0bd6a_row4_col5\" class=\"data row4 col5\" >75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row5\" class=\"row_heading level0 row5\" >Q5</th>\n",
       "      <td id=\"T_0bd6a_row5_col0\" class=\"data row5 col0\" >29%</td>\n",
       "      <td id=\"T_0bd6a_row5_col1\" class=\"data row5 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row5_col2\" class=\"data row5 col2\" >62%</td>\n",
       "      <td id=\"T_0bd6a_row5_col3\" class=\"data row5 col3\" >5%</td>\n",
       "      <td id=\"T_0bd6a_row5_col4\" class=\"data row5 col4\" >81%</td>\n",
       "      <td id=\"T_0bd6a_row5_col5\" class=\"data row5 col5\" >76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row6\" class=\"row_heading level0 row6\" >Q6</th>\n",
       "      <td id=\"T_0bd6a_row6_col0\" class=\"data row6 col0\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row6_col1\" class=\"data row6 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row6_col2\" class=\"data row6 col2\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row6_col3\" class=\"data row6 col3\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row6_col4\" class=\"data row6 col4\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row6_col5\" class=\"data row6 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row7\" class=\"row_heading level0 row7\" >Q7</th>\n",
       "      <td id=\"T_0bd6a_row7_col0\" class=\"data row7 col0\" >0%</td>\n",
       "      <td id=\"T_0bd6a_row7_col1\" class=\"data row7 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row7_col2\" class=\"data row7 col2\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row7_col3\" class=\"data row7 col3\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row7_col4\" class=\"data row7 col4\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row7_col5\" class=\"data row7 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row8\" class=\"row_heading level0 row8\" >Q8</th>\n",
       "      <td id=\"T_0bd6a_row8_col0\" class=\"data row8 col0\" >75%</td>\n",
       "      <td id=\"T_0bd6a_row8_col1\" class=\"data row8 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row8_col2\" class=\"data row8 col2\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row8_col3\" class=\"data row8 col3\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row8_col4\" class=\"data row8 col4\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row8_col5\" class=\"data row8 col5\" >100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bd6a_level0_row9\" class=\"row_heading level0 row9\" >Q9</th>\n",
       "      <td id=\"T_0bd6a_row9_col0\" class=\"data row9 col0\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row9_col1\" class=\"data row9 col1\" >100%</td>\n",
       "      <td id=\"T_0bd6a_row9_col2\" class=\"data row9 col2\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row9_col3\" class=\"data row9 col3\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row9_col4\" class=\"data row9 col4\" >50%</td>\n",
       "      <td id=\"T_0bd6a_row9_col5\" class=\"data row9 col5\" >100%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x250ce19c8c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create accuracy by question visualization\n",
    "pivot_data = []\n",
    "for dr in detailed_results:\n",
    "    pivot_data.append({\n",
    "        \"Question\": f\"Q{dr['question_id']}\",\n",
    "        \"Model\": dr['model'],\n",
    "        \"Accuracy\": dr['accuracy']\n",
    "    })\n",
    "\n",
    "pivot_df = pd.DataFrame(pivot_data)\n",
    "pivot_table = pivot_df.pivot(index=\"Question\", columns=\"Model\", values=\"Accuracy\")\n",
    "\n",
    "# Try to display with background gradient, fallback to plain table\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    styled_pivot = pivot_table.style.background_gradient(cmap='RdYlGn', vmin=0, vmax=1)\\\n",
    "        .format(\"{:.0%}\")\\\n",
    "        .set_caption(\"Accuracy by Question and Model\")\n",
    "    display(styled_pivot)\n",
    "except ImportError:\n",
    "    # Fallback: display as formatted percentage table\n",
    "    print(\"Accuracy by Question and Model\")\n",
    "    print(\"=\" * 80)\n",
    "    formatted_table = pivot_table.applymap(lambda x: f\"{x:.0%}\" if pd.notna(x) else \"N/A\")\n",
    "    display(formatted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "525f69eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to c:\\Projects\\Thesis-RAG-Langgraph-Assistant\\scripts\\evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results to JSON\n",
    "output_results = {\n",
    "    \"evaluation_date\": \"2025-12-31\",\n",
    "    \"total_questions\": len(test_questions),\n",
    "    \"models_tested\": list(MODELS.keys()),\n",
    "    \"summary\": summary_df.to_dict('records'),\n",
    "    \"detailed_results\": detailed_results\n",
    "}\n",
    "\n",
    "# Use absolute path for saving (same directory as notebook)\n",
    "output_path = r\"c:\\Projects\\Thesis-RAG-Langgraph-Assistant\\scripts\\evaluation_results.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0ae366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE DETAILED COMPARISONS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Question 1: ThÆ° viá»‡n UELlib cÃ³ bao nhiÃªu báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025)?\n",
      "\n",
      "âœ… Ground Truth: ThÆ° viá»‡n UELlib cÃ³ nguá»“n tÃ i nguyÃªn thÃ´ng tin Ä‘a dáº¡ng, vá»›i 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025).\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸŸ¢ Gemini 2.5 Flash (100%):\n",
      "   ThÆ° viá»‡n UELlib cÃ³ 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025).\n",
      "\n",
      "ğŸŸ¢ Gemini 2.5 Pro (100%):\n",
      "   ThÆ° viá»‡n UELlib cÃ³ 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025).\n",
      "\n",
      "ğŸŸ¢ Llama 3.3 70B (100%):\n",
      "   ThÆ° viá»‡n UELlib cÃ³ 66.419 báº£n tÃ i liá»‡u (cáº­p nháº­t 30/6/2025).\n",
      "\n",
      "ğŸŸ¡ GPT-OSS 120B (50%):\n",
      "   66.419 báº£n tÃ i liá»‡u.\n",
      "\n",
      "ğŸŸ¢ Qwen3 32B (100%):\n",
      "   <think>\n",
      "Okay, let's see. The user is asking how many documents the UELlib library has as of June 30, 2025. I need to check the context provided.\n",
      "\n",
      "Looking through the text, there's a mention of \"66.419...\n",
      "\n",
      "ğŸŸ¡ Kimi K2 (50%):\n",
      "   66.419 báº£n tÃ i liá»‡u.\n",
      "\n",
      "============================================================\n",
      "Question 2: Äá»‹a chá»‰ cá»§a ThÆ° viá»‡n TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t lÃ  gÃ¬?\n",
      "\n",
      "âœ… Ground Truth: Äá»‹a chá»‰: Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM. ThÆ° viá»‡n Ä‘áº·t táº¡i tÃ²a nhÃ  KTL B2 (táº§ng G, 1, 2).\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸŸ¡ Gemini 2.5 Flash (57%):\n",
      "   Äá»‹a chá»‰ cá»§a ThÆ° viá»‡n TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t lÃ : Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM.\n",
      "\n",
      "ğŸŸ¡ Gemini 2.5 Pro (57%):\n",
      "   Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM.\n",
      "\n",
      "ğŸŸ¡ Llama 3.3 70B (57%):\n",
      "   Äá»‹a chá»‰ cá»§a ThÆ° viá»‡n TrÆ°á»ng Äáº¡i há»c Kinh táº¿ - Luáº­t lÃ : Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM.\n",
      "\n",
      "ğŸ”´ GPT-OSS 120B (29%):\n",
      "   Sá»‘â€¯669â€¯Äá»—â€¯MÆ°á»i, Khu phá»‘â€¯13, phÆ°á»ng Linhâ€¯XuÃ¢n, TPâ€¯HCM.\n",
      "\n",
      "ğŸŸ¢ Qwen3 32B (86%):\n",
      "   <think>\n",
      "Okay, let's see. The user is asking for the address of the Library at UEL (University of Economics and Law). I need to look through the provided context to find the correct information.\n",
      "\n",
      "Looki...\n",
      "\n",
      "ğŸŸ¡ Kimi K2 (57%):\n",
      "   Sá»‘ 669 Äá»— MÆ°á»i, Khu phá»‘ 13, phÆ°á»ng Linh XuÃ¢n, TP. HCM.\n",
      "\n",
      "============================================================\n",
      "Question 3: NÄƒm há»c 2025-2026, UEL cÃ³ bao nhiÃªu ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c?\n",
      "\n",
      "âœ… Ground Truth: NÄƒm há»c 2025-2026, UEL cÃ³ 35 ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c, trong Ä‘Ã³ 24 chÆ°Æ¡ng trÃ¬nh dáº¡y vÃ  há»c báº±ng tiáº¿ng Viá»‡t, 09 chÆ°Æ¡ng trÃ¬nh dáº¡y vÃ  há»c báº±ng tiáº¿ng Anh vÃ  02 chÆ°Æ¡ng trÃ¬nh Co-operative Education.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸŸ¡ Gemini 2.5 Flash (67%):\n",
      "   NÄƒm há»c 2025-2026, UEL cÃ³ 35 ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c.\n",
      "\n",
      "ğŸŸ¡ Gemini 2.5 Pro (67%):\n",
      "   NÄƒm há»c 2025-2026, UEL cÃ³ 35 ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c.\n",
      "\n",
      "ğŸŸ¡ Llama 3.3 70B (67%):\n",
      "   NÄƒm há»c 2025-2026, UEL cÃ³ 35 ngÃ nh/chuyÃªn ngÃ nh Ä‘Ã o táº¡o Ä‘áº¡i há»c.\n",
      "\n",
      "ğŸ”´ GPT-OSS 120B (17%):\n",
      "   35.\n",
      "\n",
      "ğŸŸ¢ Qwen3 32B (100%):\n",
      "   <think>\n",
      "Okay, let's see. The user is asking about the number of undergraduate programs at UEL for the academic year 2025-2026. I need to look through the provided context to find the exact number.\n",
      "\n",
      "Fi...\n",
      "\n",
      "ğŸ”´ Kimi K2 (17%):\n",
      "   35 ngÃ nh/chuyÃªn ngÃ nh.\n"
     ]
    }
   ],
   "source": [
    "# Show sample detailed comparisons\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DETAILED COMPARISONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first 3 questions with all model answers\n",
    "for q_id in range(1, 4):\n",
    "    q_results = [dr for dr in detailed_results if dr['question_id'] == q_id]\n",
    "    if q_results:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {q_id}: {q_results[0]['question']}\")\n",
    "        print(f\"\\nâœ… Ground Truth: {q_results[0]['ground_truth']}\")\n",
    "        print(f\"\\n{'â”€'*60}\")\n",
    "        \n",
    "        for qr in q_results:\n",
    "            acc_emoji = \"ğŸŸ¢\" if qr['accuracy'] >= 0.7 else \"ğŸŸ¡\" if qr['accuracy'] >= 0.4 else \"ğŸ”´\"\n",
    "            print(f\"\\n{acc_emoji} {qr['model']} ({qr['accuracy']:.0%}):\")\n",
    "            print(f\"   {qr['model_answer'][:200]}...\" if len(qr['model_answer']) > 200 else f\"   {qr['model_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3f33cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ† Best Performing Model: Qwen3 32B\n",
      "   Average Accuracy: 93.7%\n",
      "   Perfect Scores: 7 out of 10\n",
      "\n",
      "ğŸ“Š Model Rankings:\n",
      "   1. Qwen3 32B: 93.7%\n",
      "   2. Gemini 2.5 Flash: 89.9%\n",
      "   3. Llama 3.3 70B: 83.0%\n",
      "   4. Gemini 2.5 Pro: 81.1%\n",
      "   5. Kimi K2: 65.4%\n",
      "   6. GPT-OSS 120B: 47.4%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = summary_df.iloc[0]\n",
    "print(f\"\\nğŸ† Best Performing Model: {best_model['Model']}\")\n",
    "print(f\"   Average Accuracy: {best_model['Average Accuracy']}\")\n",
    "print(f\"   Perfect Scores: {best_model['Perfect Scores (â‰¥90%)']} out of {best_model['Questions Tested']}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Model Rankings:\")\n",
    "for idx, (_, row) in enumerate(summary_df.iterrows(), 1):\n",
    "    print(f\"   {idx}. {row['Model']}: {row['Average Accuracy']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
